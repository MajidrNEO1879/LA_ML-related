{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOXdyfTOA9FXbrMn8vQGSRW"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 207,
      "metadata": {
        "id": "a-X0aZf84L1w"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import re\n",
        "import tiktoken\n",
        "import importlib\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data = open(r'./sample_data/harry_potter.txt', 'rt').read()\n"
      ],
      "metadata": {
        "id": "87uxUwcj5Pn3"
      },
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OMjwlKjR53QN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(raw_data))\n",
        "print(raw_data[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmH3vlmR5T7X",
        "outputId": "2fa620c1-66da-40a7-a9f8-cf855a5fd4ca"
      },
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20479\n",
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no g\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "token = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_data)\n",
        "unique_tokens = sorted(set(token))\n",
        "vocab = {}\n",
        "for integer, token in enumerate(token):\n",
        "    vocab[token] = integer\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctvhTywoDotw",
        "outputId": "32018805-e530-4246-c0a0-0472c93a5100"
      },
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'I': 9170, ' ': 9229, 'HAD': 2, 'always': 7520, 'thought': 9032, 'Jack': 4746, 'Gisburn': 5756, 'rather': 8656, 'a': 9148, 'cheap': 16, 'genius': 18, '--': 9213, 'though': 312, 'good': 606, 'fellow': 4902, 'enough': 9112, 'so': 8842, 'it': 9190, 'was': 8814, 'no': 9220, 'great': 8384, 'surprise': 5388, 'to': 9114, 'me': 9194, 'hear': 8484, 'that': 9168, ',': 9205, '': 9234, 'in': 9048, 'the': 9158, 'height': 144, 'of': 9228, 'his': 9136, 'glory': 6778, 'he': 9074, 'had': 8008, 'dropped': 5820, 'painting': 9180, 'married': 1034, 'rich': 1572, 'widow': 86, 'and': 9208, 'established': 92, 'himself': 8998, 'villa': 100, 'on': 9140, 'Riviera': 1200, '.': 9231, '(': 1533, 'Though': 112, 'would': 9000, 'have': 9002, 'been': 9020, 'Rome': 128, 'or': 7526, 'Florence': 132, ')': 1541, '\\n': 9125, '\"': 9233, 'The': 9198, 'what': 9028, 'women': 1952, 'called': 8224, 'can': 8180, 'Mrs': 8806, 'Gideon': 180, 'Thwing': 728, 'last': 7622, 'Chicago': 188, 'sitter': 190, 'deploring': 192, 'unaccountable': 196, 'abdication': 2390, 'Of': 8588, 'course': 8590, \"'\": 9217, 's': 9218, 'going': 212, 'send': 216, 'value': 220, 'my': 9142, 'picture': 8972, 'way': 8192, 'up': 9130, ';': 8879, 'but': 9214, 'don': 6004, 't': 8756, 'think': 6582, 'Mr': 4128, 'Rickham': 8442, 'loss': 260, 'Arrt': 264, 'is': 9166, 'all': 6946, 'word': 2270, 'lips': 8288, 'multiplied': 300, 'its': 2658, '_': 9175, 'rs': 306, 'as': 8202, 'they': 8222, 'were': 7510, 'reflected': 318, 'an': 6282, 'endless': 324, 'vista': 4634, 'mirrors': 330, 'And': 9044, 'not': 8708, 'only': 6994, 'Thwings': 350, 'who': 8794, 'mourned': 354, 'Had': 358, 'exquisite': 364, 'Hermia': 458, 'Croft': 1540, 'at': 8968, 'Grafton': 378, 'Gallery': 380, 'show': 8508, 'stopped': 386, 'before': 8474, 'Moon-dancers': 398, 'say': 9026, 'with': 9146, 'tears': 462, 'her': 8894, 'eyes': 8100, ':': 8799, 'We': 422, 'shall': 424, 'look': 5938, 'upon': 430, 'like': 8988, 'again': 9068, '?': 9119, 'Well': 8184, '!': 9195, 'even': 8544, 'through': 8292, 'prism': 454, 'felt': 7280, 'able': 9022, 'face': 8358, 'fact': 8052, 'equanimity': 480, 'Poor': 5684, 'made': 8682, 'him': 9102, 'fitting': 506, 'should': 8376, 'mourn': 514, 'Among': 520, 'own': 6908, 'sex': 526, 'fewer': 528, 'regrets': 530, 'heard': 3928, 'trade': 546, 'hardly': 548, 'murmur': 552, 'Professional': 556, 'jealousy': 2378, 'Perhaps': 562, 'If': 8346, 'honour': 6860, 'craft': 582, 'vindicated': 5870, 'by': 8732, 'little': 7274, 'Claude': 592, 'Nutley': 594, 'faith': 608, 'brought': 4730, 'out': 9078, 'Burlington': 620, 'very': 624, 'handsome': 4878, 'obituary': 630, 'one': 9088, 'those': 8096, 'showy': 8738, 'articles': 646, 'stocked': 648, 'random': 652, 'technicalities': 654, 'won': 668, 'whom': 676, 'compared': 680, 'resolve': 942, 'being': 6862, 'apparently': 702, 'irrevocable': 704, 'discussion': 708, 'gradually': 710, 'died': 7006, 'predicted': 732, 'price': 738, 'Gisburns': 744, 'went': 8572, 'It': 8674, 'till': 6572, 'three': 764, 'years': 7702, 'later': 1850, 'few': 784, 'weeks': 1330, 'idling': 790, 'suddenly': 7008, 'occurred': 804, 'wonder': 5170, 'why': 6000, 'given': 9004, 'On': 830, 'reflection': 832, 'really': 4364, 'tempting': 844, 'problem': 846, 'To': 1714, 'accuse': 852, 'wife': 7608, 'too': 8648, 'easy': 866, 'fair': 870, 'sitters': 8026, 'denied': 878, 'solace': 882, 'saying': 886, 'dragged': 1056, 'down': 8926, 'For': 6044, 'such': 7146, 'existed': 926, 'nearly': 930, 'year': 934, 'after': 5804, 'taken': 7638, 'might': 4406, 'be': 8850, 'since': 9182, 'liked': 8658, 'ease': 976, 'because': 6632, 'didn': 8594, 'want': 8722, 'go': 7270, 'hard': 7500, 'prove': 1012, 'if': 9014, 'she': 8884, 'equally': 1068, 'Miss': 1538, 'contended': 1078, 'failed': 7440, 'lift': 3716, 'led': 1102, 'back': 8942, 'easel': 6974, 'put': 9060, 'brush': 4922, 'into': 8062, 'hand': 9138, 'vocation': 1136, 'for': 9192, 'But': 8696, 'appeared': 1154, 'disdained': 1160, 'interesting': 1460, 'find': 1180, 'desultory': 1192, 'life': 4758, 'lends': 1202, 'itself': 1204, 'purely': 1210, 'academic': 1212, 'speculations': 1214, 'having': 4710, 'Monte': 1868, 'Carlo': 1870, 'caught': 7574, 'glimpse': 1242, 'balustraded': 1250, 'terraces': 1252, 'between': 7504, 'pines': 1258, 'myself': 6898, 'borne': 1268, 'thither': 1270, 'next': 8392, 'day': 9036, 'found': 4188, 'couple': 1288, 'tea': 1292, 'beneath': 8958, 'their': 8064, 'palm-trees': 1298, 'welcome': 1312, 'genial': 1318, 'ensuing': 1328, 'claimed': 1336, 'frequently': 1340, 'hostess': 2870, 'point': 3740, 'could': 8350, 'fullest': 1386, 'reassurance': 1388, 'just': 8270, 'may': 1416, 'pardoned': 1420, 'bull': 1424, 'surrounded': 1456, 'fostered': 1470, 'art': 9230, 'reared': 1484, 'hot-house': 1490, 'adulation': 1496, 'therefore': 1506, 'instructive': 1508, 'note': 7680, 'effect': 1516, 'deadening': 1522, 'atmosphere': 1524, 'mediocrity': 1528, 'quote': 1536, 'mentioned': 1560, 'immediately': 1582, 'perceptible': 1584, 'husband': 8896, 'extracting': 1594, 'from': 7664, 'this': 3920, 'circumstance': 1600, 'delicate': 3096, 'substantial': 1608, 'satisfaction': 7130, 'rule': 1624, 'people': 4374, 'scorn': 1634, 'money': 1636, 'get': 6978, 'most': 6730, 'elegant': 1658, 'disdain': 1660, 'big': 1670, 'balance': 1672, 'enabled': 1674, 'appearance': 1684, 'perfect': 1688, 'good-breeding': 1690, 'transmute': 1696, 'objects': 3102, 'luxury': 1710, 'latter': 2384, 'must': 5366, 'add': 1726, 'remained': 1732, 'relatively': 1734, 'indifferent': 1736, 'buying': 1746, 'Renaissance': 1748, 'bronzes': 1750, 'eighteenth-century': 2660, 'pictures': 5680, 'discrimination': 1762, 'bespoke': 1766, 'amplest': 1770, 'resources': 1772, 'Money': 1780, 'excuse': 1786, 'beauty': 1922, 'circulation': 1798, 'axioms': 1812, 'laid': 9134, 'across': 1820, 'Sevres': 1824, 'silver': 1828, 'exquisitely': 1834, 'appointed': 1836, 'luncheon-table': 1838, 'when': 7822, 'run': 1862, 'over': 3320, 'beaming': 1884, 'added': 6284, 'enlightenment': 1898, 'morbidly': 1910, 'sensitive': 1912, 'every': 7714, 'form': 1918, 'fate': 1946, 'things': 8900, 'set': 6968, 'extenuation': 1980, 'What': 5166, 'struck': 2262, 'now': 6474, 'first': 8744, 'time': 8162, 'resented': 2010, 'tone': 4934, 'seen': 2698, 'often': 2030, 'basking': 2034, 'under': 6672, 'similar': 2038, 'tributes': 2040, 'conjugal': 2048, 'robbed': 2054, 'them': 7924, 'savour': 2062, 'No': 2066, 'oddly': 2072, 'became': 3412, 'apparent': 2082, 'fond': 2100, 'see': 8402, 'absurdity': 2124, 'seemed': 8304, 'wincing': 8878, 'attitude': 2142, 'object': 5020, 'garlands': 2152, 'incense': 2156, 'My': 7262, 'dear': 6884, 've': 6194, 'chucked': 2682, 'stuff': 2190, 'about': 6856, 'Victor': 2274, 'Grindle': 9184, 'protest': 2218, 'rose': 3358, 'table': 2232, 'strolled': 2236, 'onto': 2240, 'sunlit': 2244, 'terrace': 2914, 'glanced': 4094, 'becoming': 2288, 'man': 8824, 'moment': 5514, 'hour': 2330, 'younger': 2336, 'artist': 2338, 'said': 8628, 'formed': 2348, 'friend': 2356, 'feet': 2360, 'wondered': 2368, 'tinge': 2374, 'underlay': 2380, 'mysterious': 2388, 'event': 2412, 'Dubarry': 3360, 'drawing-rooms': 5630, 'begun': 2430, 'display': 3600, 'Grindles': 2440, 'turned': 7772, 'lingered': 2466, 'give': 8686, 'lump': 2474, 'sugar': 2478, 'spaniel': 2484, 'dining-room': 2490, 'Why': 7384, 'has': 2502, 'asked': 4790, 'abruptly': 2520, 'She': 8654, 'raised': 2528, 'eyebrows': 2532, 'hint': 2538, 'good-humoured': 6238, 'Oh': 6248, 'doesn': 3876, 'you': 8510, 'know': 8326, 'enjoy': 2590, 'quite': 5852, 'simply': 8626, 'looked': 8964, 'spacious': 2618, 'white-panelled': 2620, 'room': 6390, 'famille-verte': 2632, 'vases': 2636, 'repeating': 2638, 'tones': 2642, 'pale': 3350, 'damask': 2650, 'curtains': 3392, 'pastels': 2662, 'faded': 2668, 'frames': 2670, 'Has': 2678, 'haven': 2694, 'single': 2702, 'house': 8240, 'A': 7720, 'slight': 2720, 'shade': 2722, 'constraint': 2726, 'crossed': 2728, 'open': 2738, 'countenance': 2740, 'ridiculous': 2826, 'modesty': 2834, 'He': 9126, 'says': 8156, 're': 8332, 'fit': 2774, 'sent': 6944, 'away': 4968, 'except': 2796, 'portrait': 8714, 'keep': 2814, 'upstairs': 2816, 'His': 7606, 'curiosity': 6056, 'growing': 2850, 'bean-stalk': 2856, 'persuasively': 2864, 'your': 4542, 'almost': 2906, 'timorously': 2908, 'where': 8328, 'lounging': 2924, 'hooded': 2930, 'chair': 2932, 'lit': 2938, 'cigar': 2942, 'drawn': 2946, 'Russian': 2950, 'deerhound': 2952, 'head': 8946, 'knees': 2962, 'come': 6638, 'while': 2976, 'looking': 6122, 'laugh': 9150, 'tried': 7422, 'hide': 3008, 'nervousness': 3012, 'followed': 7216, 'marble': 3028, 'Emperors': 3030, 'hall': 3036, 'wide': 3046, 'stairs': 3048, 'terra-cotta': 3052, 'nymphs': 3054, 'poised': 3056, 'among': 8892, 'flowers': 3060, 'each': 3064, 'landing': 3066, 'In': 3072, 'dimmest': 3076, 'corner': 3078, 'boudoir': 3084, 'amid': 3088, 'profusion': 3092, 'distinguished': 3100, 'hung': 8886, 'familiar': 3114, 'oval': 3116, 'canvases': 3118, 'inevitable': 3126, 'garlanded': 3128, 'frame': 3144, 'mere': 3136, 'outline': 3138, 'past': 4112, 'drew': 3168, 'window-curtains': 3174, 'moved': 8650, 'aside': 3180, 'jardiniere': 3186, 'full': 3190, 'pink': 3194, 'azaleas': 3196, 'pushed': 6314, 'arm-chair': 8932, 'stand': 9202, 'here': 6344, 'manage': 4376, 'mantel-piece': 5014, 'wouldn': 8754, 'let': 8758, 'stay': 6642, 'Yes': 8784, 'ever': 5736, 'strain': 3314, 'Usually': 3324, 'place': 3410, 'central': 3342, 'panel': 3344, 'yellow': 3352, 'drawing-room': 3364, 'monumental': 3372, 'placed': 6364, 'took': 3384, 'light': 6156, 'old': 5080, 'Venetian': 3398, 'more': 7374, 'modest': 3408, 'better': 6080, 'yet': 3786, 'grew': 4856, 'accustomed': 3434, 'half-light': 3440, 'characteristic': 3448, 'qualities': 3450, 'came': 3452, 'hesitations': 3460, 'disguised': 3462, 'audacities': 3466, 'tricks': 7432, 'prestidigitation': 3476, 'which': 6228, 'consummate': 3488, 'skill': 3490, 'managed': 3496, 'divert': 3500, 'attention': 7482, 'real': 3508, 'business': 3510, 'some': 8734, 'pretty': 3522, 'irrelevance': 3524, 'detail': 3528, 'presenting': 3540, 'neutral': 3544, 'surface': 3546, 'work': 7780, 'forming': 3554, 'inevitably': 3568, 'background': 3572, 'lent': 3584, 'herself': 4158, 'unusual': 3592, 'degree': 4052, 'false': 3606, 'virtuosity': 3608, 'strongest': 3628, 'admirers': 3638, 'represented': 3650, 'part': 3658, 'swelling': 3664, 'muscles': 3668, 'congesting': 3674, 'veins': 3678, 'balancing': 3684, 'straddling': 3688, 'straining': 3692, 'reminded': 3698, 'circus-clown': 3706, 'ironic': 3710, 'efforts': 3712, 'feather': 3720, 'met': 7058, 'short': 3732, 'demand': 3744, 'lovely': 3748, 'woman': 5686, 'painted': 8872, 'strongly': 3760, 'tired': 5082, 'sweetly': 3780, 'lose': 3792, 'atom': 3796, 'sweetness': 3802, 'pardonable': 3842, 'pride': 4272, 'corrected': 3864, 'other': 5704, 'count': 3880, 'destroyed': 3888, 'Destroyed': 3900, 'follow': 3916, 'clue': 3922, 'footstep': 3932, 'saw': 7816, 'threshold': 3946, 'As': 3952, 'stood': 9128, 'there': 9216, 'hands': 3964, 'pockets': 3970, 'velveteen': 3976, 'coat': 3978, 'thin': 3984, 'brown': 4646, 'waves': 3988, 'hair': 3992, 'white': 4002, 'forehead': 4004, 'lean': 4010, 'sunburnt': 4012, 'cheeks': 4866, 'furrowed': 4016, 'smile': 7318, 'lifted': 4026, 'tips': 4030, 'self-confident': 4036, 'moustache': 4038, 'same': 4060, 'quality': 4072, 'cleverer': 4078, 'than': 4908, 'deprecatingly': 4100, 'travelled': 4110, 'wanted': 5866, 'began': 7266, 'excusing': 4156, 'shrugged': 4164, 'shoulders': 4168, 'still': 9178, 'smiling': 4174, 'long': 4194, 'ago': 4196, 'lightly': 4206, 'then': 7798, 'passing': 4214, 'arm': 4218, 'mine': 8936, 'Come': 4228, 'rest': 6648, 'showed': 4252, 'kind': 9226, 'naive': 4268, 'suburban': 4270, 'bath-rooms': 4278, 'speaking-tubes': 4284, 'dress-closets': 4290, 'trouser-presses': 4296, 'complex': 4302, 'simplifications': 4304, 'millionaire': 4310, 'domestic': 4314, 'economy': 4316, 'whenever': 4322, 'paid': 4328, 'expected': 4332, 'tribute': 4334, 'throwing': 4342, 'chest': 4348, 'how': 8512, 'live': 4380, 'without': 8876, 'end': 8774, 'foreseen': 4410, 'Only': 9156, 'spite': 4460, 'charming': 4478, 'disarming': 4484, 'longed': 4526, 'cry': 4560, 'Be': 4536, 'dissatisfied': 4538, 'leisure': 4512, 'once': 9212, 'diagnosis': 4572, 'suffered': 4574, 'unexpected': 4980, 'check': 4580, 'This': 4588, 'lair': 4596, 'leading': 4608, 'dark': 4616, 'plain': 7980, 'florid': 4632, 'square': 4642, 'leathery': 4650, 'effects': 4658, 'bric-a-brac': 4666, 'none': 4670, 'air': 4676, 'posing': 6894, 'reproduction': 4684, 'weekly': 4692, 'above': 8974, 'least': 4702, 'sign': 4704, 'used': 4714, 'studio': 4720, 'home': 4732, 'absolute': 4740, 'finality': 4742, 'break': 4750, 'Don': 8124, 'dabble': 6008, 'paint': 8636, 'any': 7892, 'trace': 4804, 'activity': 4810, 'Never': 4888, 'briefly': 4828, 'Or': 4836, 'water-colour': 4838, 'etching': 4842, 'confident': 4852, 'dim': 4858, 'paled': 4868, 'sunburn': 4880, 'd': 9018, 'never': 8010, 'touched': 4918, 'told': 8832, 'flash': 4944, 'anything': 8450, 'else': 8836, 'instinctively': 5826, 'embarrassed': 4974, 'discovery': 4982, 'eye': 4998, 'fell': 5000, 'small': 5006, 'breaking': 5022, 'oak': 5028, 'panelling': 5030, 'Jove': 6614, 'sketch': 7582, 'donkey': 8692, 'standing': 5088, 'rain': 5094, 'wall': 7596, 'By': 6612, 'Stroud': 8996, 'cried': 5122, 'silent': 5132, 'close': 7324, 'behind': 7320, 'breathing': 5152, 'quickly': 5158, 'Made': 5174, 'dozen': 5180, 'lines': 5182, 'everlasting': 6692, 'foundations': 8118, 'You': 5734, 'lucky': 5196, 'chap': 5198, 'did': 8718, 'answered': 5396, 'slowly': 5222, 'gave': 5234, 'Ah': 5538, 'knew': 9110, 'Strouds': 9200, 'inflexible': 5280, 'hermit': 5282, 'dead': 8098, 'When': 9084, 'much': 6446, 'amazement': 5380, 'escape': 5382, 'deprecating': 6222, 'awful': 5418, 'simpleton': 5420, 'Her': 5438, 'idea': 8662, 'done': 8728, 'fashionable': 5526, 'painter': 5528, 'ah': 5462, 'poor': 8264, 'surest': 5480, 'proclaiming': 5486, 'greatness': 5490, 'forcing': 5494, 'purblind': 5502, 'public': 8032, 'Was': 5554, 'history': 7694, 'That': 7366, 'believed': 6268, 'gloried': 5594, 'couldn': 8632, 'bear': 5644, 'varnishing': 5656, 'days': 5928, 'near': 9100, 'fragment': 5698, 'groping': 7790, 'fragments': 5706, 'whole': 7692, 'curious': 5762, 'happened': 6418, 'voice': 5824, 'insensible': 5854, 'irony': 9160, 'laughed': 6300, 'threw': 8058, 'There': 7698, 'thing': 9090, 'forced': 5960, 'cured': 5986, 'reason': 6036, 'idle': 6054, 'companion': 6062, 'serious': 6070, 'desire': 6072, 'understand': 8260, 'wish': 6090, 'tell': 8598, 'twirling': 6136, 'fingers': 6142, 'cigarette': 6146, 'forgotten': 6152, 'Suddenly': 6160, 'toward': 6166, 'suspected': 6198, 'loathing': 6204, 'gesture': 6224, 'negatived': 6232, 'shrug': 6240, 'care': 6258, 'straw': 6262, 'tie': 6286, 'us': 6652, 'slightly': 6302, 'bitterness': 6308, 'deep': 6512, 'arm-chairs': 6324, 'forward': 6326, 'make': 6336, 'yourself': 6338, 'comfortable': 6340, 'are': 7700, 'cigars': 6350, 'elbow': 6372, 'continued': 6376, 'wander': 6380, 'stopping': 6394, 'How': 6414, 'five': 6432, 'minutes': 6434, 'take': 6444, 'longer': 6448, 'happen': 9210, 'remember': 6844, 'surprised': 6478, 'pleased': 6482, 'got': 8846, 'gone': 6548, 'stream': 6554, 'echoed': 6558, 'usual': 6562, 'platitudes': 6564, 'half': 8158, 'failure': 6768, 'left': 6930, 'ourselves': 6660, 'swept': 6664, 'along': 6666, 'high': 6682, 'current': 7734, 'off': 9118, 'egregious': 6732, 'mood': 6734, 'Lord': 6742, 'forgive': 6744, 'pathos': 6754, 'career': 6764, 'crowned': 6772, 'meant': 6796, 'do': 7970, 'nothing': 6808, 'stammer': 6830, 'something': 6832, 'poverty': 6838, 'getting': 8710, 'prodigious': 6852, 'phrase': 6854, 'oh': 8430, 'princely': 6878, 'Then': 7158, 'alone': 9204, 'traps': 8566, 'advance': 6954, 'twenty-four': 6996, 'hours': 6998, 'heart': 7670, 'disease': 7016, 'preliminary': 7032, 'destruction': 7038, 'clear': 7046, 'untouched': 7050, 'twice': 7066, 'insignificant': 7082, 'dingy': 7086, 'Now': 7090, 'superb': 7102, 'glad': 7134, 'merely': 7126, 'aesthetic': 7128, 'subject': 7864, 'strange': 7162, 'life-likeness': 7164, 'affect': 7170, 'queerly': 7174, 'blocked': 7180, 'watching': 8278, 'sensation': 7212, 'working': 7258, 'strokes': 7264, 'wild': 7276, 'nervous': 7282, 'uncertain': 7286, 'Once': 7294, 'grayish': 7326, 'beard': 8298, 'secret': 7394, 'amusing': 7348, 'holding': 7354, 'exasperated': 7368, 'worth': 7396, 'twenty': 7398, 'dashed': 7408, 'canvas': 7414, 'furiously': 7416, 'bravura': 7430, 'crumbled': 7448, 'wasn': 7460, 'bits': 7470, 'distract': 7478, 'kept': 7490, 'passages': 7502, 'Those': 7508, 'ones': 7514, 'shirked': 7522, 'covered': 8046, 'lying': 7536, 'lies': 7554, 'sight': 7576, 'hanging': 7590, 'bed': 7602, 'afterward': 7614, 'shaking': 7644, 'Devonshire': 7660, 'recovering': 7662, 'previous': 7668, 'attack': 7672, 'Just': 7676, 'tells': 7688, 'patient': 7706, 'scornful': 7708, 'persistence': 7710, 'line': 7716, 'swum': 7728, 'learned': 7742, 'mighty': 7746, 'up-stream': 7748, 'stroke': 7974, 'muddling': 7794, 'possessed': 7860, 'absorbed': 7868, 'recreated': 7874, 'They': 7902, 'hadn': 7904, 'born': 7910, 'adopted': 7922, 'Hang': 7944, 'another': 7972, 'truth': 7982, 'known': 8012, 'splash': 8040, 'colour': 8044, 'faces': 8066, 'medium': 8094, 'straight': 8110, 'tottering': 8116, 'underneath': 8120, 'talking': 8138, 'foreign': 8142, 'language': 8144, 'fluently': 8150, 'wants': 8170, 'lay': 8272, 'watched': 8212, 'technique': 8230, 'collapsed': 8234, 'cards': 8244, 'sneer': 8254, 'quietly': 8276, 'gray': 8296, 'question': 9056, 'Are': 8318, 'sure': 8322, 'coming': 8820, 'greatest': 8394, 'grace': 8416, 'minute': 8438, 'earth': 8454, 'alive': 8554, 'late': 8532, 'll': 8506, 'packed': 8560, 'Greek': 8616, 'romantic': 8670, 'terribly': 8702, 'upset': 8704, 'At': 8742, 'afraid': 8750, 'wits': 8770, 'suggested': 8778, 'started': 8796, 'somebody': 8834, 'true': 8852, 'flung': 8922, 'clasping': 8952, 'arms': 8956, 'chimney-piece': 8978, 'fancy': 8992, 'answer': 9050, 'half-mechanically': 9062, 'Begin': 9066, 'flashed': 9076, 'brings': 9094, 'anywhere': 9098, 'leave': 9116, 'shoulder': 9144, 'am': 9174, 'doing': 9188, 'exterminating': 9222, 'our': 9224}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Token_maker:\n",
        "  def __init__(self,data):\n",
        "    self.string_to_int = data\n",
        "    self.int_to_string = {}\n",
        "    for s, i in data.items():\n",
        "        self.int_to_string[i] = s\n",
        "        #encoding the input to tokens\n",
        "  def string_token(self, text):\n",
        "      tokens = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "      token_ids = []\n",
        "      for t in tokens:\n",
        "            if t in self.string_to_int:\n",
        "                token_ids.append(self.string_to_int[t])\n",
        "      return token_ids\n",
        "      #token to text\n",
        "  def token_string(self, ids):\n",
        "      words = []  #list to store reconstructed words\n",
        "      for i in ids:\n",
        "            if i in self.int_to_string:\n",
        "                words.append(self.int_to_string[i])\n",
        "\n",
        "      text = \" \".join(words)  # join words with spaces\n",
        "      text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "      return text\n"
      ],
      "metadata": {
        "id": "_wkbzWjbZ3h-"
      },
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_tests = Token_maker(token_id)\n",
        "token_tests.string_token(\"For a famous place, it was very dark and shabby. A few old women were sitting in a corner, drinking tiny glasses of sherry. One of them was smoking a long pipe. A little man in a top hat was talking to the old bartender, who was quite bald and looked like a toothless walnut. The\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRYoqBCZb6xq",
        "outputId": "497d9998-ad33-4e77-dfa5-cc9136b5f4c9"
      },
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[38,\n",
              " 2,\n",
              " 118,\n",
              " 2,\n",
              " 2,\n",
              " 776,\n",
              " 8,\n",
              " 0,\n",
              " 2,\n",
              " 588,\n",
              " 2,\n",
              " 1080,\n",
              " 2,\n",
              " 1068,\n",
              " 2,\n",
              " 316,\n",
              " 2,\n",
              " 160,\n",
              " 2,\n",
              " 10,\n",
              " 0,\n",
              " 2,\n",
              " 14,\n",
              " 2,\n",
              " 440,\n",
              " 2,\n",
              " 729,\n",
              " 2,\n",
              " 1115,\n",
              " 2,\n",
              " 1091,\n",
              " 2,\n",
              " 2,\n",
              " 571,\n",
              " 2,\n",
              " 118,\n",
              " 2,\n",
              " 293,\n",
              " 8,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 725,\n",
              " 2,\n",
              " 10,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 725,\n",
              " 2,\n",
              " 993,\n",
              " 2,\n",
              " 1080,\n",
              " 2,\n",
              " 2,\n",
              " 118,\n",
              " 2,\n",
              " 642,\n",
              " 2,\n",
              " 10,\n",
              " 0,\n",
              " 2,\n",
              " 14,\n",
              " 2,\n",
              " 638,\n",
              " 2,\n",
              " 659,\n",
              " 2,\n",
              " 571,\n",
              " 2,\n",
              " 118,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 1080,\n",
              " 2,\n",
              " 977,\n",
              " 2,\n",
              " 1019,\n",
              " 2,\n",
              " 991,\n",
              " 2,\n",
              " 729,\n",
              " 2,\n",
              " 8,\n",
              " 0,\n",
              " 2,\n",
              " 1100,\n",
              " 2,\n",
              " 1080,\n",
              " 2,\n",
              " 816,\n",
              " 2,\n",
              " 2,\n",
              " 160,\n",
              " 2,\n",
              " 646,\n",
              " 2,\n",
              " 631,\n",
              " 2,\n",
              " 118,\n",
              " 2,\n",
              " 2,\n",
              " 10,\n",
              " 0,\n",
              " 2,\n",
              " 96]"
            ]
          },
          "metadata": {},
          "execution_count": 212
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_tests.token_string([56, 2, 533, 2, 670, 2, 990, 2, 70, 10, 0, 2, 41, 2, 1080])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "RroQcPlxfLdJ",
        "outputId": "e446b4a8-84ff-4925-9fff-5a11623f1970"
      },
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I   have   mentioned   that   Mrs.    Gisburn   was'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 213
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a3n1wWbocUU",
        "outputId": "4497c61f-c010-4083-f243-c08640ee181f"
      },
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tiktoken version: 0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "Lx8RLmb7onN_"
      },
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode(\"kjewlkfjwlkejflkwjelkfjweklfj\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9acMjUUoq1i",
        "outputId": "1022ec1e-95c3-4279-db31-f357ef547c5d"
      },
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[42421,\n",
              " 413,\n",
              " 75,\n",
              " 74,\n",
              " 69,\n",
              " 73,\n",
              " 40989,\n",
              " 365,\n",
              " 73,\n",
              " 2704,\n",
              " 46265,\n",
              " 73,\n",
              " 417,\n",
              " 74,\n",
              " 69,\n",
              " 73,\n",
              " 732,\n",
              " 74,\n",
              " 1652,\n",
              " 73]"
            ]
          },
          "metadata": {},
          "execution_count": 216
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch = []\n",
        "text_1 = \"hello world, harry potter is here\"\n",
        "text_2 = \"the harry potter, books are good\"\n",
        "batch.append(torch.tensor(tokenizer.encode(text_1)))\n",
        "batch.append(torch.tensor(tokenizer.encode(text_2)))\n",
        "batch = torch.stack(batch, dim=0)\n",
        "print(batch)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkcdS9lA1nc9",
        "outputId": "482e1b46-a33f-4a6d-8903-f779c539222f"
      },
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[31373,   995,    11,  3971,   563,  1787,   353,   318,   994],\n",
            "        [ 1169,  3971,   563,  1787,   353,    11,  3835,   389,   922]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]\n",
        "\n",
        "\n",
        "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
        "                         stride=128, shuffle=True, drop_last=True,\n",
        "                         num_workers=0):\n",
        "\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "KtZsvwe9JXh-"
      },
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,    # Vocabulary size\n",
        "    \"context_length\": 1024, # Context length\n",
        "    \"emb_dim\": 768,         # Embedding dimension\n",
        "    \"n_heads\": 12,          # Number of attention heads\n",
        "    \"n_layers\": 12,         # Number of layers\n",
        "    \"drop_rate\": 0.0,       # Dropout rate\n",
        "    \"qkv_bias\": False       # Query-Key-Value bias\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "id": "cbMBq9gIJ7fz"
      },
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DummyGPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        # Use a placeholder for TransformerBlock\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "        # Use a placeholder for LayerNorm\n",
        "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(\n",
        "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
        "        )\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        x = tok_embeds + pos_embeds\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "class DummyTransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        # A simple placeholder\n",
        "\n",
        "    def forward(self, x):\n",
        "        # This block does nothing and just returns its input.\n",
        "        return x\n",
        "\n",
        "\n",
        "class DummyLayerNorm(nn.Module):\n",
        "    def __init__(self, normalized_shape, eps=1e-5):\n",
        "        super().__init__()\n",
        "        # The parameters here are just to mimic the LayerNorm interface.\n",
        "\n",
        "    def forward(self, x):\n",
        "        # This layer does nothing and just returns its input.\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "6x0Q4A-EJrid"
      },
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "dummy_model = DummyGPTModel(GPT_CONFIG_124M)\n",
        "logits = dummy_model(batch)\n",
        "print(logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rI58SjeVJbH6",
        "outputId": "40f17552-98a0-4629-ffc5-3fea0a27dc32"
      },
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.4730,  0.9534, -0.1357,  ..., -0.7575, -0.3694, -0.0449],\n",
            "         [-0.1691,  1.1632,  0.3353,  ..., -0.2662,  1.3028,  0.6348],\n",
            "         [ 0.9966,  1.0554,  0.4811,  ...,  1.2790,  0.3781, -0.3264],\n",
            "         ...,\n",
            "         [ 0.6231, -0.1986, -0.6411,  ...,  0.8808,  1.2568,  0.0774],\n",
            "         [ 0.3620,  0.0164, -0.5234,  ..., -0.5839, -1.2104, -0.6022],\n",
            "         [-0.2765, -0.2690, -1.3648,  ..., -1.3578, -1.2946, -0.1746]],\n",
            "\n",
            "        [[-0.0740,  0.3409, -0.2020,  ..., -0.9521, -0.0818, -0.9191],\n",
            "         [ 0.0391,  0.8940, -0.0082,  ...,  0.6117, -0.1145,  1.4780],\n",
            "         [ 0.1847,  1.3747,  0.4571,  ...,  0.1258,  1.1706, -0.1840],\n",
            "         ...,\n",
            "         [ 0.4712,  0.3927, -0.1322,  ...,  0.5540,  0.2059,  0.5349],\n",
            "         [ 0.0755, -0.3041,  0.7430,  ..., -0.5034, -0.3173, -0.9137],\n",
            "         [ 0.4292, -0.4279, -0.8059,  ...,  0.7091, -1.0550, -0.5708]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ]
    }
  ]
}